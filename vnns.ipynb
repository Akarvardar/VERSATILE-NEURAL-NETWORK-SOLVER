{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=darkblue> VERSATILE NEURAL NETWORK SOLVER (VNNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.optimize as op\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import timeit\n",
    "import time\n",
    "from IPython.display import display\n",
    "import matplotlib.cm as cm\n",
    "import random\n",
    "import ast\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from cycler import cycler\n",
    "mpl.rcParams['axes.prop_cycle'] = cycler(color='rbykgmc')\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['axes.grid'] = 'True'\n",
    "mpl.rcParams['font.size'] = 13\n",
    "mpl.rcParams['legend.fontsize'] = 'small'\n",
    "mpl.rcParams['figure.figsize'] = [5.0, 4.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PreProcess():  \n",
    "    \"\"\" Processes input data for subsequent deep learning operations. Starting from a raw .csv file, it allows to \n",
    "        normalize, add polynomial features, and partition the data into training and validation sets. \n",
    "    Parameters:\n",
    "        datafile -- .csv file including feature matrix and target vector\n",
    "        normalize -- boolean that allows data normalization if True (default = False)        \n",
    "        polydeg -- boolean that allows feature matrix to be augmented with polynomial terms if > 1 (int, default = 1)        \n",
    "        partition -- boolean that allows partitioning of the dataset into training and validation sets if True \n",
    "                    (default = False)\n",
    "        fts -- fraction of training samples used for partition. Fixed at 0.7. \n",
    "    Attributes:\n",
    "        original_labels_ -- list containing original class labels from the datafile\n",
    "        numclass_ -- number of classes associated with the target vector (int, > 1)\n",
    "        mu_ -- array containing normalization mean of feature matrix (num_features,)  \n",
    "        sigma_ -- array containing normalization standard deviation of feature matrix (num_features,)\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, datafile, normalize=False, polydeg=1, partition=False):       \n",
    "        self.datafile = datafile\n",
    "        self.normalize = normalize\n",
    "        self.polydeg = polydeg\n",
    "        self.partition = partition\n",
    "        self.fts = 0.7\n",
    "    \n",
    "    def read_datafile(self, dfile=''): \n",
    "        \"\"\" Reads datafile (a csv file or alike with or without a header) and converts it to a data matrix. \n",
    "        Parameters:\n",
    "            dfile -- .csv datafile (optional). If not specified self.datafile is being read. \n",
    "        Returns:\n",
    "            data_array -- data matrix (num_samples, num_features+1)        \n",
    "        \"\"\"\n",
    "        file = dfile if dfile else self.datafile\n",
    "        c = pd.read_csv(file, nrows = 1).columns # read the first row\n",
    "        try:\n",
    "            np.array(c).astype(float) # if cols can be converted to float w/o error, that means there is no header\n",
    "            data_array = pd.read_csv(file, header = None).values \n",
    "        except ValueError:            \n",
    "            data_array = pd.read_csv(file).values\n",
    "        return data_array\n",
    "          \n",
    "    def data_array_to_xy(self, data_array):\n",
    "        \"\"\" Partitions the data matrix to a feature matrix and a target vector with standard labels.        \n",
    "        Parameters:\n",
    "            data_array -- data matrix (num_samples, num_features+1)      \n",
    "        Returns:\n",
    "            X -- feature matrix (num_samples, num_features)\n",
    "            y -- target vector containing standard (int >= 0) class labels (number of training samples, 1)        \n",
    "        \"\"\" \n",
    "        X = data_array[:,:-1].astype(float) # all cols except the last one \n",
    "        y_raw = data_array[:,-1][:,None] # last column of data array\n",
    "        y = self.standardize_class_labels(y_raw)\n",
    "        return X,y\n",
    "    \n",
    "    def standardize_class_labels(self, y_raw): \n",
    "        \"\"\" Converts class labels to int >= 0.         \n",
    "        Parameters:\n",
    "            y_raw -- target vector containing raw class labels (num_samples, 1)       \n",
    "        Returns:\n",
    "            y -- target vector containing standard (int >= 0) class labels (num_samples, 1)        \n",
    "        \"\"\"\n",
    "        self.original_labels_ = np.unique(y_raw).tolist()\n",
    "        self.numclass_ = len(self.original_labels_)\n",
    "        assert self.numclass_ >= 2\n",
    "        y = y_raw\n",
    "        for i in range (self.numclass_):\n",
    "            A = np.where(y_raw == self.original_labels_[i])[0] # locations of elements with i'th  label in y_raw,  \n",
    "            y[A] = i # assign i to those locations ==> 0,1,2, follows the alphabetical order\n",
    "        y = y.astype(int)\n",
    "        return y\n",
    "       \n",
    "    def normalize_features(self, X):\n",
    "        \"\"\" Normalizes the feature matrix.     \n",
    "        Parameters:\n",
    "            X -- feature matrix (num_samples, num_features)      \n",
    "        Returns:\n",
    "            Xnorm -- normalized feature matrix where each column has a mean of ~0 and std deviation of 1 \n",
    "                    (num_samples, num_features)  \n",
    "        \"\"\"\n",
    "        m,n = X.shape # m: number of samples, n: number of features\n",
    "        Xnorm = np.zeros((m,n))\n",
    "        self.mu_ = np.mean(X,axis = 0)\n",
    "        self.sigma_ = np.std(X,axis = 0)\n",
    "        for j in range (n):\n",
    "            Xnorm[:,j] = (X[:,j] - self.mu_[j])/self.sigma_[j]\n",
    "        return Xnorm\n",
    "    \n",
    "    def add_polynomial_features(self, X): #ACKNOWLEDGMENTS: SCIKIT-LEARN\n",
    "        \"\"\" Generates features for polynomial regression.\n",
    "        Parameters:\n",
    "            X -- feature matrix (num_samples, num_features)\n",
    "        Returns:\n",
    "            Xpoly -- new feature matrix augmented with polynomial features (dimension depends on the polynomial \n",
    "                     degree & num_features)              \n",
    "        \"\"\"\n",
    "        poly = PolynomialFeatures(self.polydeg, include_bias=False)\n",
    "        Xpoly = poly.fit_transform(X)\n",
    "        return Xpoly\n",
    "        \n",
    "    def partition_training_set(self, X, y, seedno):\n",
    "        \"\"\" Partitions the data set into training and validation sets.     \n",
    "        Parameters:\n",
    "            X -- feature matrix (num_samples, num_features) \n",
    "            y -- target vector containing standard (int >= 0) class labels (num_samples, 1) \n",
    "            seedno -- seed for randomization of partition into training and validation sets (int)\n",
    "        Returns:\n",
    "            Xtr -- training feature matrix (int(self.fts*num_samples), num_features) \n",
    "            ytr -- training target vector (int(self.fts*num_samples), 1)\n",
    "            Xval -- validation feature matrix (1-int(fts*num_samples), num_features) \n",
    "            yval -- validation target vector (1-int(fts*num_samples), 1)\n",
    "        \"\"\"\n",
    "        data_arr = np.column_stack((X,y))\n",
    "        m,n = X.shape\n",
    "        np.random.seed(seedno)\n",
    "        np.random.shuffle(data_arr)                   \n",
    "        train_set = data_arr[:int(m*self.fts)+1] \n",
    "        valid_set = data_arr[int(m*self.fts)+1:]\n",
    "        Xtr = train_set[:,0:n]\n",
    "        ytr = train_set[:,n][:,None].astype(int)\n",
    "        Xval = valid_set[:,0:n]\n",
    "        yval = valid_set[:,n][:,None].astype(int)\n",
    "        if len(np.unique(ytr)) < self.numclass_ or len(np.unique(yval)) < self.numclass_:\n",
    "            print(\"training and/or validation set has less than the orginal number of classes\")\n",
    "            assert(len(np.unique(ytr)) == self.numclass_ and len(np.unique(yval)) == self.numclass_)\n",
    "            # stop if partitioned sets do not include at least one sample from each class\n",
    "        return Xtr, ytr, Xval, yval\n",
    "    \n",
    "    def process_training_set(self, seedno=0):\n",
    "        \"\"\" Combines reading input file, label standardization, feature normalization, polynomial feature addition, \n",
    "            and partitioning of input training data. Returned vector/matrix dimensions depends on whether there is \n",
    "            polynomial regression(which would increase the number of features) and partition (which would change the \n",
    "            number of training and validation samples.)\n",
    "        Parameters:\n",
    "            seedno -- seed for randomization of partition into training and validation sets (int)         \n",
    "        Returns:\n",
    "            X -- pre-processed training feature matrix (training final num_samples, final num_features) \n",
    "            y -- pre-processed training target vector (training final num_samples, 1)\n",
    "            Xval -- pre-processed validation feature matrix (validation final num_samples, final num_features) \n",
    "            yval -- pre-processed validation target vector (validation final num_samples, 1)\n",
    "        \"\"\"  \n",
    "        data_array = self.read_datafile()\n",
    "        X,y = self.data_array_to_xy(data_array)\n",
    "        Xval = None # overwritten below if self.partition = True\n",
    "        yval = None\n",
    "        if self.normalize == True:\n",
    "            X = self.normalize_features(X) \n",
    "        if self.polydeg > 1:\n",
    "            X = self.add_polynomial_features(X) # In case both normalization and polynomial regression were \n",
    "                                                # set to True, normalization is implemented first.\n",
    "        if self.partition == True:\n",
    "            X,y,Xval,yval = self.partition_training_set(X,y,seedno) # X, y = Xtr, ytr\n",
    "        return X, y, Xval, yval\n",
    "\n",
    "    def process_test_set(self, test_file,mu_tr='', sigma_tr=''):\n",
    "        \"\"\" Combines reading, label standardization, feature normalization, and polynomial feature addition for the \n",
    "            test set.  \n",
    "        Parameters:\n",
    "            test_file -- a separate data file never used by the network during training\n",
    "            mu_tr -- array containing normalization mean from the training set, optional in case training set is \n",
    "                     actually not normalized(num_features,)  \n",
    "            sigma_tr -- array containing normalization standard deviation from the training set, optional in case \n",
    "                        training set is actually not normalized(num_features,)  \n",
    "        Returns:\n",
    "            Xtest -- pre-processed test feature matrix (test num_samples, final num_features) \n",
    "            ytest -- pre-processed test target vector (test num_samples, 1)\n",
    "        \"\"\"  \n",
    "        test_array = self.read_datafile(test_file)\n",
    "        Xtest,ytest = self.data_array_to_xy(test_array)\n",
    "        mtest,ntest = Xtest.shape\n",
    "        if self.normalize == True:\n",
    "            for j in range (ntest):                \n",
    "                Xtest[:,j] = (Xtest[:,j] - mu_tr[j])/sigma_tr[j]\n",
    "        if self.polydeg > 1:\n",
    "            Xtest = self.add_polynomial_features(Xtest) \n",
    "        return Xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeepSolve():    \n",
    "    \"\"\" Computes parameters for neural networks with arbitrary number of hidden layers and units in each layer. \n",
    "        Makes predictions based on calculated parameters and reports their accuracy. Accommodates multi-class \n",
    "        classification. A custom gradient descent method (with \"auto\" learning-rate option) as well as \n",
    "        scipy.optimize.minimize methods are available to solve the parameters. \n",
    "    Parameters:\n",
    "        method -- batch solver based on either a custom gradient descent implementation ('GD' which is the default) \n",
    "                  or other built-in algorithm implementations under scipy.optimize.minimize (such as 'TNC','CG',\n",
    "                  'SLSQP','BFGS','L-BFGS-B') \n",
    "                  See https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html.\n",
    "        initialize -- type of initialization for network parameters. \"zeros\" results in initial parameters being all \n",
    "                      zero. \"deep\" (default) results in He initialization\n",
    "        learning_rate -- learning rate for gradient descent. Effective only if method = 'GD'. Can be either a float \n",
    "                         > 0 or 'auto' (default) which would allow 'GD' to auto-select the learning rate\n",
    "        lamda -- regularization parameter (float >= 0, default = 0)\n",
    "        maxiter -- max number of iterations (int >= 1, default = 1000)\n",
    "        hidden units -- list showing number of units in each hidden layer. As an example, hidden_units = [20,15,10] \n",
    "                        refers to a network with three hidden layers and 20 units in the first layer and so on. \n",
    "                        hidden_units = [] (default) means there are no hidden units (in which case the operation is \n",
    "                        simple logistic regression)\n",
    "        L -- total number of layers in the network, equals to number of hidden layers + 2 to account for input and \n",
    "             output layers (value is tied to parameter hidden_units)\n",
    "        minlr -- minimum allowable learning rate if parameter learning_rate is set to 'auto'. Effective only if \n",
    "                 method = 'GD'. Fixed at 1e-8\n",
    "        crit -- convergence criterion for 'GD' (so effective only if method = 'GD'.) 'GD' converges when cost decrease \n",
    "                between two subsequent iterations divided by the actual cost becomes less than the parameter crit. \n",
    "                Fixed at 2e-5\n",
    "    Attributes:\n",
    "        Jh_ -- array containing the cost value at each iteration  including the initial cost (numiter+1,)\n",
    "        message_ -- solver result, e.g., \"Converged\" (string)\n",
    "        niter_ -- number of iterations (int > 0)\n",
    "        lrfinal_ -- auto-selected value of learning rate for method = 'GD' and learning_rate = 'auto' (float)\n",
    "        timetofit_ -- execution time taken by solver (sec)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, method='GD', initialize='deep', learning_rate='auto', lamda=0, maxiter=1000, hidden_units=[]):\n",
    "        self.hidden_units = hidden_units\n",
    "        self.method = method\n",
    "        self.lamda = lamda\n",
    "        self.maxiter = maxiter\n",
    "        self.initialize = initialize\n",
    "        self.learning_rate = learning_rate \n",
    "        self.L = len(hidden_units)+2       \n",
    "        self.minlr = 1e-8\n",
    "        self.crit = 2e-5\n",
    "        \n",
    "    def relu(self, x):\n",
    "        \"\"\" Rectifying linear unit activation function.\n",
    "        Parameters:\n",
    "            x -- array (any size)\n",
    "        Returns:\n",
    "            r -- relu(x) = x if x > 0 else 0\n",
    "        \"\"\"\n",
    "        r =  x * (x > 0)\n",
    "        return r\n",
    "\n",
    "    def gradrelu(self, x): \n",
    "        \"\"\" Gradient of rectifying linear unit activation function.\n",
    "        Parameters:\n",
    "            x -- array (any size)\n",
    "        Returns:\n",
    "            gr -- gradient of relu(x) = 1 if x > 0 else 0\n",
    "        \"\"\"\n",
    "        gr = (x > 0) * 1\n",
    "        return gr\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\" Sigmoid activation function.\n",
    "        Parameters:\n",
    "            x -- array (any size)\n",
    "        Returns:\n",
    "            s -- sigmoid(x)\n",
    "        \"\"\"\n",
    "        s = 1/(1 + np.exp(-x))\n",
    "        return s\n",
    "\n",
    "    def add_bias(self, X):\n",
    "        \"\"\" Inserts a vector of ones as the first column to the designated matrix.\n",
    "        Parameters:\n",
    "            X -- feature matrix (num_samples,num_features)\n",
    "        Returns:\n",
    "            X1 -- feature matrix augmented with a bias vector of ones as the first column (num_samples,num_features+1)\n",
    "        \"\"\"\n",
    "        X1 = np.insert(X,0,1,axis=1)\n",
    "        return X1\n",
    "\n",
    "    def convert_to_one_hot(self, y): \n",
    "        \"\"\"Converts target vector to one-hot matrix if number of classes > 2.\n",
    "        Parameters:\n",
    "            y -- target vector containing standard (int >= 0) class labels (num_samples, 1) \n",
    "        Returns:\n",
    "            ymc -- y itself (num_samples, 1) or one-hot matrix (num_samples,num_classes)\n",
    "        \"\"\"\n",
    "        ymc = y\n",
    "        classes = np.unique(y)\n",
    "        numc = len(classes)\n",
    "        if numc > 2:\n",
    "            ymc = (y==classes[0])*1\n",
    "            for i in range (1,numc):\n",
    "                ymc = np.column_stack((ymc,(y==classes[i])*1)) \n",
    "        return ymc\n",
    "\n",
    "    def set_layer_sizes(self, X1, y):\n",
    "        \"\"\" Sets layer sizes including input, hidden, and output units.\n",
    "        Parameters:\n",
    "            X1 -- feature matrix augmented with a bias vector of ones as the first column (num_samples,num_features+1)                 \n",
    "            y -- target vector containing standard (int >= 0) class labels (num_samples, 1) \n",
    "        Returns:\n",
    "            layer_sizes -- list holding number of units in each layer\n",
    "        \"\"\"\n",
    "        num_input_units = X1.shape[1]-1 # X1 is preferred to X as parameter to speed-up parameter fitting\n",
    "        classes = np.unique(y)\n",
    "        num_out_units = len(classes) if len(classes) > 2 else 1 # if there are only two classes, there is no need for \n",
    "                                                                # two ouput units whose outputs would be complementary\n",
    "                                                                # with probabilities p and 1-p\n",
    "        layer_sizes = [num_input_units] + self.hidden_units + [num_out_units] \n",
    "        return layer_sizes\n",
    "\n",
    "    def initialize_parameters(self, layer_sizes):\n",
    "        \"\"\" Initializes parameters theta (i.e., bias vector concatenated with weight matrix.) \n",
    "        Parameters:\n",
    "            layer_sizes -- list holding number of units in each layer                 \n",
    "        Returns:\n",
    "            theta_init -- dictionary holding initialized network parameters \n",
    "                          keys: int > 0, total number of keys: num_layers - 1\n",
    "                          dimensions for each key: (num_units_next, num_units_prev+1)\n",
    "        \"\"\" \n",
    "        theta_init = {} \n",
    "        np.random.seed(3)\n",
    "        for l in range(1, self.L):\n",
    "            if self.initialize == \"deep\":  \n",
    "                w = np.random.randn(layer_sizes[l], layer_sizes[l-1])/np.sqrt(layer_sizes[l-1]) # He initialization\n",
    "            if self.initialize == \"zeros\":\n",
    "                w = np.zeros((layer_sizes[l], layer_sizes[l-1]))\n",
    "            b = np.zeros((layer_sizes[l], 1)) # bias always initialized to zero    \n",
    "            theta_init[l] = np.column_stack((b,w))\n",
    "        return theta_init\n",
    "\n",
    "    def fwd_prop(self, theta, X1):\n",
    "        \"\"\" Calculates pre-activation and activation matrices for each layer of the network through a forward \n",
    "            propagation from input to output. Relu activation is used at all layers except the output where sigmoid \n",
    "            activation is used.\n",
    "        Parameters:\n",
    "            theta -- dictionary holding network parameters\n",
    "                     keys: int > 0, total number of keys: num_layers - 1\n",
    "                     dimensions for each key: (num_units_next, num_units_prev+1)\n",
    "            X1 -- feature matrix augmented with a bias vector of ones as the first column (num_samples,num_features+1)                \n",
    "        Returns:\n",
    "            AL -- activation matrix at the output (1 or num_classes, num_samples)\n",
    "            A1 -- dictionary holding activation matrices with bias added as the first row\n",
    "                  keys: int > 0, total number of keys: num_layers - 1\n",
    "                  dimensions for each key: (num_units+1, num_samples)        \n",
    "        \"\"\" \n",
    "        A = {}\n",
    "        A1 = {}\n",
    "        Z = {}    \n",
    "        A1[1] = X1.T\n",
    "        k = 2\n",
    "        while k <= self.L:\n",
    "            Z[k] = np.dot(theta[k-1], A1[k-1])\n",
    "            if k < self.L:               \n",
    "                A[k] = self.relu(Z[k])\n",
    "            elif k == self.L:\n",
    "                A[k] = self.sigmoid(Z[k])\n",
    "            if k < self.L:\n",
    "                A1[k] = np.insert(A[k],0,1,axis=0) \n",
    "            k = k+1\n",
    "        AL = A[self.L]\n",
    "        return Z,AL,A1\n",
    "    \n",
    "    def back_prop(self, theta, y, AL, Z):\n",
    "        \"\"\" Calculates the error matrices for each layer of the network except the input through a backward \n",
    "            propagation from output to input. \n",
    "        Parameters:\n",
    "            theta -- dictionary holding network parameters\n",
    "                     keys: int > 0, total number of keys: num_layers - 1\n",
    "                     dimensions for each key: (num_units_next, num_units_prev+1)\n",
    "            y -- target vector containing standard (int >= 0) class labels (num_samples, 1) \n",
    "            AL -- activation matrix at the output (1 or num_classes, num_samples)\n",
    "            Z -- dictionary holding pre-activation matrices\n",
    "                 keys: int > 1, total number of keys: num_layers - 1\n",
    "                 dimensions for each key: (num_units, num_samples)             \n",
    "        Returns:            \n",
    "            dZ -- dictionary holding error matrices associated with each layer except the input\n",
    "                  keys: int > 1, total number of keys: num_layers - 1\n",
    "                  dimensions for each key: (num_units, num_samples)\n",
    "        \"\"\"\n",
    "        dZ = {}\n",
    "        l = self.L\n",
    "        ymc = self.convert_to_one_hot(y)  \n",
    "        while l >=2 :\n",
    "            if l == self.L:\n",
    "                dZ[l] = AL - ymc.T \n",
    "            elif l < self.L:\n",
    "                theta_r_l = theta[l][:,1:] #strip out the bias vector from the first column\n",
    "                theta_r_T_l = theta_r_l.T\n",
    "                dA_l = np.dot(theta_r_T_l,dZ[l+1])\n",
    "                grZ_l = self.gradrelu(Z[l])                \n",
    "                dZ[l] = dA_l*grZ_l\n",
    "            l -= 1\n",
    "        return dZ\n",
    "        \n",
    "    def cost_nn(self, theta, X1, y):\n",
    "        \"\"\" Calculates cross-entropy cost including the regularization term.\n",
    "        Parameters:\n",
    "            theta -- dictionary holding network parameters\n",
    "                     keys: int > 0, total number of keys: num_layers - 1\n",
    "                     dimensions for each key: (num_units_next, num_units_prev+1)\n",
    "            X1 -- feature matrix augmented with a bias vector of ones as the first column (num_samples,num_features+1)\n",
    "            y -- target vector containing standard (int >= 0) class labels (num_samples, 1) \n",
    "        Returns:\n",
    "            cost -- cost based on cross-entropy loss (-ylog(AL)-(1-y)log(1-AL))) + regularization term (float)\n",
    "        \"\"\"\n",
    "        eps = 1e-323 # prevents logarithm of zero in Jdiag below\n",
    "        m = len(y)\n",
    "        ymc = self.convert_to_one_hot(y)\n",
    "        Z, AL, A1 = self.fwd_prop(theta,X1)\n",
    "        Jdiag =-np.einsum('ij,ji->i',ymc.T,np.log10(AL.T+eps))/m-np.einsum('ij,ji->i',(1-ymc).T,np.log10(1-AL.T+eps))/m\n",
    "        Jnoreg = np.sum(Jdiag) # Cost without regularization \n",
    "        # einsum formulation above replaces the for loop below in case of more than two classes (and also works for two \n",
    "        # classes):  \n",
    "        # Jnoreg = 0\n",
    "        # for k in range (0,len(np.unique(y))):\n",
    "        #    yy = ymc[:,k][:,None]      \n",
    "        #    hh = AL.T[:,k][:,None]\n",
    "        #    J_lam0 = (-np.dot(yy.T,np.log10(hh+eps))-np.dot((1-yy).T,np.log10(1-hh+eps)))/m\n",
    "        #    Jnoreg = Jnoreg + J_lam0[0][0]\n",
    "        Jreg = 0\n",
    "        for i in range (1,self.L):\n",
    "            theta_r = theta[i][:,1:]\n",
    "            Jreg += (self.lamda/(2*m))*sum(sum(theta_r**2)) # Regularization term\n",
    "        cost = Jnoreg + Jreg\n",
    "        return cost  \n",
    "\n",
    "    def grad_nn(self, theta, X1, y):\n",
    "        \"\"\" Calculates the gradient of cost (including the regularization term) with respect to theta.\n",
    "        Parameters:\n",
    "            theta -- dictionary holding network parameters\n",
    "                     keys: int > 0, total number of keys: num_layers - 1\n",
    "                     dimensions for each key: (num_units_next, num_units_prev+1)\n",
    "            X1 -- feature matrix augmented with a bias vector of ones as the first column (num_samples,num_features+1)\n",
    "            y -- target vector containing standard (int >= 0) class labels (num_samples, 1) \n",
    "        Returns:\n",
    "            grad -- dictionary holding the gradient of cost for each theta\n",
    "                    keys: int > 0, total number of keys: num_layers - 1\n",
    "                    dimensions for each key: (num_units_next, num_units_prev+1)\n",
    "        \"\"\"\n",
    "        m = y.shape[0]        \n",
    "        grad = {}            \n",
    "        Z,AL,A1 = self.fwd_prop(theta,X1)\n",
    "        dZ = self.back_prop(theta,y,AL,Z)\n",
    "        l = self.L\n",
    "        while l >=2 :\n",
    "            grad_noreg = np.dot(dZ[l],A1[l-1].T)/m # unregularized term\n",
    "            grad_reg = np.array(theta[l-1]*(self.lamda/m)) # regularized term\n",
    "            grad_reg[:,0] = 0 # don't regularize the bias term\n",
    "            grad[l-1] = grad_noreg + grad_reg\n",
    "            l -= 1\n",
    "        return grad\n",
    "    \n",
    "    def fit_gdnn(self, X, y):\n",
    "        \"\"\" Finds network parameters theta by gradient descent. Learning rate can either be set by the user or be \n",
    "            auto-selected.\n",
    "        Parameters:\n",
    "            X -- feature matrix (num_samples,num_features)\n",
    "            y -- target vector containing standard (int >= 0) class labels (num_samples, 1) \n",
    "        Returns:\n",
    "            theta -- dictionary holding network parameters\n",
    "                     keys: int > 0, total number of keys: num_layers - 1\n",
    "                     dimensions for each key: (num_units_next, num_units_prev+1)\n",
    "        \"\"\"           \n",
    "        X1 = self.add_bias(X)\n",
    "        layer_sizes = self.set_layer_sizes(X1,y)\n",
    "        theta_init = self.initialize_parameters(layer_sizes) \n",
    "        cost_init = self.cost_nn(theta_init,X1,y) \n",
    "        theta = theta_init\n",
    "        self.Jh_ = cost_init\n",
    "        alp = 10 if self.learning_rate == 'auto' else self.learning_rate # initial learning rate in \"auto\" mode\n",
    "        i = 0\n",
    "        while i < self.maxiter and alp > self.minlr:        \n",
    "            i = i + 1 \n",
    "            grad = self.grad_nn(theta,X1,y)\n",
    "            for j in range (1,len(layer_sizes)):\n",
    "                theta[j] -= alp*grad[j] # update theta \n",
    "            self.Jh_ = np.append(self.Jh_,self.cost_nn(theta,X1,y)) # storing cost at each step tends to increase \n",
    "                                                                    # execution time significantly\n",
    "            if (self.Jh_[i-1] - self.Jh_[i])<= 0:\n",
    "                if self.learning_rate == 'auto':\n",
    "                    alp = alp*0.3 # in \"auto\" mode, reduce learning rate by 0.3 until cost decreases monotonically\n",
    "                    theta_init = self.initialize_parameters(layer_sizes) \n",
    "                    cost_init = self.cost_nn(theta_init,X1,y) \n",
    "                    theta = theta_init # re-initialization is more effective than continuing from where it diverged\n",
    "                    self.Jh_ = cost_init          \n",
    "                    i = 0    \n",
    "            elif 0 <(self.Jh_[i-1] - self.Jh_[i])/ self.Jh_[i] < self.crit: # stop when delta cost/cost<convergence \n",
    "                                                                            # threshold\n",
    "                self.message_ = \"Converged\"\n",
    "                break\n",
    "        self.niter_ = i\n",
    "        self.lrfinal_ = alp \n",
    "        if self.niter_ == self.maxiter:\n",
    "            self.message_ = \"Max number of iterations reached\" \n",
    "        elif self.lrfinal_ < self.minlr:\n",
    "            self.message_ = \"No convergence (learning rate too low)\" \n",
    "        return theta\n",
    "    \n",
    "    def unroll(self, dict):\n",
    "        \"\"\" Maps dictionary values into a column vector.\n",
    "        Parameters:\n",
    "            dict -- a dictionary where each value can be a matrix\n",
    "        Returns:\n",
    "            dict_unrolled -- vector storing dictionary values in unrolled form (num_dict_elements,1) \n",
    "        \"\"\"\n",
    "        dict_unrolled = np.array([])\n",
    "        keys = list(dict.keys())        \n",
    "        for k in keys:\n",
    "            t = np.array(dict[k])\n",
    "            dict_unrolled = np.append(dict_unrolled,np.ravel(t))        \n",
    "        dict_unrolled = dict_unrolled[:,None]\n",
    "        return dict_unrolled\n",
    "      \n",
    "    def dict_dims(self, dict):\n",
    "        \"\"\" Holds dimensions of matrices that are values to a dictionary.\n",
    "        Parameters:\n",
    "            dict --  a dictionary where each value can be a matrix\n",
    "        Returns:\n",
    "            dims -- a matrix with each column storing the shape of a matrix associated with the values of \n",
    "                    a dictionary (2, num_keys) \n",
    "        \"\"\"\n",
    "        keys = list(dict.keys())\n",
    "        dims = np.array(dict[keys[0]].shape)\n",
    "        for i in range (1, len(dict.keys())):\n",
    "            dims = np.row_stack((dims,np.array(dict[keys[i]].shape)))\n",
    "        dims = dims.T.reshape((2,len(dict.keys())))\n",
    "        return dims   \n",
    "    \n",
    "    def roll(self, dict_unrolled,dims):\n",
    "        \"\"\" Restores original dictionary.\n",
    "        Parameters:\n",
    "            dict_unrolled -- vector storing authentic dictionary values in unrolled form (num_dict_elements,1)\n",
    "            dims -- a matrix with each column storing the shape of a matrix associated with the values of \n",
    "                    an authentic dictionary (2, num_keys)\n",
    "        Returns:\n",
    "            dict_authentic --  authentic dictionary with keys: int > 0  \n",
    "        \"\"\"\n",
    "        dict_authentic = {}\n",
    "        len_prev = 0\n",
    "        for i in range(0,dims.shape[1]):\n",
    "            num_elements_i = dims[0,i]*dims[1,i]\n",
    "            w = dict_unrolled[len_prev : len_prev + num_elements_i] # slice dict_unr vector to extract elements \n",
    "            dict_authentic[i+1] = w.reshape((dims[0,i],dims[1,i])) # reshape those according to dims\n",
    "            len_prev = len_prev + num_elements_i\n",
    "        return dict_authentic\n",
    "        \n",
    "    def fit_scipynn(self, X, y):\n",
    "        \"\"\" Finds network parameters theta by using scipy's built-in minimization methods. Local functions defined \n",
    "            within the main function allow to accommodate the use of unrolled network parameters and gradients as \n",
    "            required by scipy.optimize.minimize.\n",
    "        Parameters:\n",
    "            X -- feature matrix (num_samples,num_features)\n",
    "            y -- target vector containing standard (int >= 0) class labels (num_samples, 1) \n",
    "        Returns:\n",
    "            theta -- dictionary holding network parameters\n",
    "                     keys: int > 0, total number of keys: num_layers - 1\n",
    "                     dimensions for each key: (num_units_next, num_units_prev+1)\n",
    "        \"\"\" \n",
    "        X1 = self.add_bias(X)\n",
    "        layer_sizes = self.set_layer_sizes(X1,y)\n",
    "        theta_init = self.initialize_parameters(layer_sizes) \n",
    "        cost_init = self.cost_nn(theta_init,X1,y) \n",
    "        self.Jh_ = cost_init          \n",
    "        theta_init_unrolled = self.unroll(theta_init)\n",
    "        theta_dims = self.dict_dims(theta_init)\n",
    "        def cost_nn_red(theta_unrolled, X1, y):\n",
    "            \"\"\"Similar to cost_nn method except that theta_unrolled replaces theta as parameter \"\"\"\n",
    "            theta = self.roll(theta_unrolled,theta_dims)\n",
    "            cost = self.cost_nn(theta,X1,y)\n",
    "            return cost\n",
    "        def grad_nn_red(theta_unrolled, X1, y):\n",
    "            \"\"\"Similar to grad_nn method except that theta_unrolled replaces theta as parameter and grad_unrolled \n",
    "               is returned instead of grad\"\"\"\n",
    "            theta = self.roll(theta_unrolled,theta_dims)\n",
    "            grad = self.grad_nn(theta, X1, y)\n",
    "            grad_unrolled = self.unroll(grad).T[0]\n",
    "            return grad_unrolled\n",
    "        def track_cost(theta_unrolled):\n",
    "            \"\"\"Stores cost values at each iteration\"\"\"\n",
    "            self.Jh_ = np.append(self.Jh_,cost_nn_red(theta_unrolled,X1,y))\n",
    "            return self       \n",
    "        fmin = op.minimize(fun = cost_nn_red, x0 = theta_init_unrolled, args = (X1, y), callback = track_cost,\n",
    "                           method = self.method, jac = grad_nn_red, options={'maxiter': self.maxiter})       \n",
    "        self.niter_ = fmin.nit\n",
    "        self.message_ = fmin.message     \n",
    "        theta = self.roll(fmin.x,theta_dims)     \n",
    "        return theta\n",
    "       \n",
    "    def fit_nn(self, X, y):\n",
    "        \"\"\" Finds network parameters theta either by gradient descent or using scipy's built-in minimization methods \n",
    "            Also keeps track of execution time.\n",
    "        Parameters:\n",
    "            X -- feature matrix (num_samples,num_features)\n",
    "            y -- target vector containing standard (int >= 0) class labels (num_samples, 1) \n",
    "        Returns:\n",
    "            theta -- dictionary holding network parameters\n",
    "                     keys: int > 0, total number of keys: num_layers - 1\n",
    "                     dimensions for each key: (num_units_next, num_units_prev+1)\n",
    "        \"\"\" \n",
    "        tic = timeit.default_timer()\n",
    "        theta = self.fit_gdnn(X,y) if self.method == \"GD\" else self.fit_scipynn(X,y)\n",
    "        toc = timeit.default_timer()        \n",
    "        self.timetofit_ = np.round(toc-tic,6)\n",
    "        return theta\n",
    "            \n",
    "    def predict(self, theta, X):\n",
    "        \"\"\" Predicts labels based on network parameters theta.\n",
    "        Parameters:\n",
    "            theta -- dictionary holding network parameters\n",
    "                     keys: int > 0, total number of keys: num_layers - 1\n",
    "                     dimensions for each key: (num_units_next, num_units_prev+1)\n",
    "            X -- feature matrix (num_samples,num_features)\n",
    "        Returns:\n",
    "            y_hat -- array of predicted output in standard form of int >= 0 (num_samples,)\n",
    "        \"\"\"\n",
    "        keys = list(theta.keys())\n",
    "        num_row_last_theta = theta[max(keys)].shape[0]\n",
    "        num_class = 2 if num_row_last_theta == 1 else num_row_last_theta\n",
    "        X1 = self.add_bias(X)\n",
    "        Z,AL,A1= self.fwd_prop(theta,X1)\n",
    "        if num_class == 2:\n",
    "            pred = np.where(AL.T >= 0.5, 1, 0)\n",
    "        elif num_class > 2:                       \n",
    "            pred = AL.T.argmax(axis=1).reshape((len(X),1))\n",
    "        y_hat = np.squeeze(pred)\n",
    "        return y_hat\n",
    "    \n",
    "    def score(self, y_hat, y):\n",
    "        \"\"\" Measures the accuracy of predictions by comparing them with the true labels.\n",
    "        Parameters:\n",
    "            y_hat -- array of predicted output containing standard (int >= 0) class labels (num_samples,)\n",
    "            y -- target vector containing standard (int >= 0) class labels (num_samples, 1) \n",
    "        Returns:\n",
    "            accuracy -- number of accurately predicted labels over the number of all predictions (float, min:0, max:1)\n",
    "        \"\"\"\n",
    "        ind_where_y_equals_yhat = np.where(y_hat == np.ravel(y))[0]\n",
    "        accuracy = len(ind_where_y_equals_yhat)/len(y)\n",
    "        return accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeepCombine():    \n",
    "    \"\"\" Combines two classes, PreProcess() and DeepSolve(), and complements them with plotting and report generating \n",
    "        functions. It absorbs all intermediate steps to generate the results (weights, scores..) as directly as \n",
    "        possible starting from the raw dataset.\n",
    "    Parameters:\n",
    "        datafile -- .csv file including feature matrix and target vector\n",
    "        normalize -- boolean that allows data normalization if True (default = False)        \n",
    "        polydeg -- boolean that allows feature matrix to be augmented with polynomial terms if > 1 (int, default = 1)        \n",
    "        partition -- boolean that allows partitioning of the dataset into training and validation sets if True \n",
    "                    (default = False)\n",
    "        method -- batch solver based on either a custom gradient descent implementation ('GD' which is the default) \n",
    "                  or other built-in algorithm implementations under scipy.optimize.minimize (such as 'TNC','CG',\n",
    "                  'SLSQP','BFGS','L-BFGS-B') \n",
    "                  See https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html.\n",
    "        initialize -- type of initialization for network parameters. \"zeros\" results in initial parameters being all \n",
    "                      zero. \"deep\" (default) results in He initialization\n",
    "        learning_rate -- learning rate for gradient descent. Effective only if method = 'GD'. Can be either a float \n",
    "                         > 0 or 'auto' (default) which would allow 'GD' to auto-select the learning rate\n",
    "        lamda -- regularization parameter (float >= 0, default = 0)\n",
    "        maxiter -- max number of iterations (int >= 1, default = 1000)\n",
    "        hidden units -- list showing number of units in each hidden layer. As an example, hidden_units = [20,15,10] \n",
    "                        refers to a network with three hidden layers and 20 units in the first layer and so on. \n",
    "                        hidden_units = [] (default) means there are no hidden units (in which case the operation is \n",
    "                        simple logistic regression)\n",
    "        pp -- instance of Class PreProcess()\n",
    "        ds -- instance of Class DeepSolve()\n",
    "        colors -- list of colors enabling to color match the scatter and decision boundary plots \n",
    "    \"\"\"\n",
    "    def __init__(self, datafile, normalize=False, polydeg=1, partition=False, \n",
    "                 method='GD', initialize = 'deep', learning_rate='auto', lamda=0, maxiter=10000, hidden_units=[]):\n",
    "        self.pp = PreProcess(datafile,normalize, polydeg, partition)        \n",
    "        self.ds = DeepSolve(method, initialize, learning_rate, lamda, maxiter, hidden_units)\n",
    "        self.colors = ['blue','orange','red','lime','purple','yellow','cyan','magenta','black','purple']\n",
    "\n",
    "    def plt_input(self,X,y):\n",
    "        \"\"\" Generates a 2D plot mapping color-coded class labels to the first two features X1 and X2.\n",
    "        Parameters:\n",
    "            X -- feature matrix (num_samples,num_features)\n",
    "            y -- target vector containing standard (int >= 0) class labels (num_samples, 1) \n",
    "        \"\"\"\n",
    "        for i in range(self.pp.numclass_): \n",
    "            z = np.where(y==i)[0]\n",
    "            plt.scatter(X[z,0],X[z,1],facecolor = self.colors[i], linewidth = 1, label=self.pp.original_labels_[i],\n",
    "                        edgecolor = \"black\",s = 40)\n",
    "        plt.legend (loc='best')\n",
    "        plt.xlabel('X1')\n",
    "        plt.ylabel('X2')\n",
    "        plt.title('training dataset')\n",
    "                     \n",
    "    def plot_J(self): \n",
    "        \"\"\" Plots normalized cost function (J/J_init) vs iterations.\n",
    "        \"\"\"\n",
    "        iter = range(1, self.ds.niter_+2) # +2: +1 due to starting w/1, other +1 due to inclusion of Jh[0]\n",
    "                                          # start from 1 to accommodate x scale in log\n",
    "        norm_cost_max = self.ds.Jh_[0]/self.ds.Jh_[-1]\n",
    "        norm_cost = self.ds.Jh_/self.ds.Jh_[0]       \n",
    "        if norm_cost_max > 10 and self.ds.niter_>10:\n",
    "            plt.loglog(iter, norm_cost ,'-', linewidth = 3) # use log scale if range > 1 decade\n",
    "        elif norm_cost_max <= 10 and self.ds.niter_> 10:\n",
    "            plt.semilogx(iter, norm_cost ,'-', linewidth = 3)\n",
    "        elif norm_cost_max > 10 and self.ds.niter_ <= 10:\n",
    "            plt.semilogy(iter, norm_cost ,'-', linewidth = 3)\n",
    "        elif  norm_cost_max <= 10 and self.ds.niter_ <= 10:\n",
    "            plt.plot(iter, norm_cost ,'-', linewidth = 3)\n",
    "        plt.xlim(xmin=1)\n",
    "        plt.xlabel('Number of Iterations') #in fact this is numiter + 1\n",
    "        plt.ylabel('Normalized cost') # Jh/Jh[0]    \n",
    "        plt.grid(True,which='both')\n",
    "        plt.title('Norm. cost vs num. iterations')\n",
    "        plt.show()\n",
    "            \n",
    "    def plt_output(self, X, y, theta, y_hat, tr=True):  \n",
    "        \"\"\" Generates a 2D contour plot (for the first two features X1 and X2) identifying regions belonging to \n",
    "            different classes. It also overlays input scatter plot on top of the contour plot and shows erroneous \n",
    "            predictions if any. \n",
    "        Parameters:\n",
    "            X -- feature matrix (num_samples,num_features)\n",
    "            y -- target vector containing standard (int >= 0) class labels (num_samples, 1)             \n",
    "            theta -- dictionary holding network parameters\n",
    "                     keys: int > 0, total number of keys: num_layers - 1\n",
    "                     dimensions for each key: (num_units_next, num_units_prev+1)\n",
    "            y_hat -- array of predicted output in standard form of int >= 0 (num_samples,)\n",
    "            tr -- boolean indicating the nature of dataset (default = True)\n",
    "        \"\"\"\n",
    "        ### 1. Determine axis boundaries and step size \n",
    "        numstep = 250\n",
    "        extfactor = 0.1 \n",
    "        extra1 = (np.max(X[:,0]) - np.min(X[:,0]))*extfactor\n",
    "        extra2 = (np.max(X[:,1]) - np.min(X[:,1]))*extfactor\n",
    "        x1 = (np.arange(np.min(X[:,0]) - extra1, np.max(X[:,0]) + extra1, \n",
    "                        (np.max(X[:,0])-np.min(X[:,0])+2*extra1)/numstep))\n",
    "        x2 = (np.arange(np.min(X[:,1]) - extra2, np.max(X[:,1]) + extra2, \n",
    "                        (np.max(X[:,1])-np.min(X[:,1])+2*extra2)/numstep))\n",
    "        ### 2. Predict the output for each (x1[i], x2[j]) coordinate       \n",
    "        pred = np.zeros((len(x1), len(x2)))\n",
    "        for i in range (0,len(x1)):    \n",
    "            for j in range (0,len(x2)):\n",
    "                xx = np.array([x1[i],x2[j]]).reshape((1,2)) # select the coordinate on the 2D map\n",
    "                xxx = self.pp.add_polynomial_features(xx) # add polynomial features if self.pp.polydeg > 1\n",
    "                if theta[1].shape[1] != xxx.shape[1]+1:\n",
    "                    print (\"\\nnumber of input features > 2 is not allowed w/o polynomial regression\\n\")\n",
    "                assert theta[1].shape[1] == xxx.shape[1]+1\n",
    "                pred[i,j]  = self.ds.predict(theta,xxx) # for each coordinate, predict the output based on theta   \n",
    "        ### 3. Contour setup           \n",
    "        num_classes = len(np.unique(y))\n",
    "        levels = np.linspace(0,num_classes-1,num_classes+1)\n",
    "        plt.contour(x1, x2, pred.T, levels = levels, linewidths=0.5, colors='k') # plots the decision boundary\n",
    "        cont = plt.contourf(x1, x2, pred.T, levels = levels,colors = self.colors,alpha=0.4) # color different regions \n",
    "        ### 4. Overlay input scatter plot and show errors\n",
    "        self.plt_input(X,y) \n",
    "        error_index = np.where(y_hat!= y.T)[1]\n",
    "        plt.scatter(X[error_index,0],X[error_index,1],marker='o', facecolor='None', edgecolor='black', s=40, \n",
    "                    linewidth=2, label='errors') # show errors\n",
    "        plt.legend(bbox_to_anchor=(1.3,1.03))\n",
    "        plt.title('decision boundary on the tr set') if tr==True else plt.title('decision boundary on the test set')  \n",
    "        plt.show() \n",
    "    \n",
    "    def report(self, X, y, yval='', score_tr='', score_val='', cost_val=''):\n",
    "        \"\"\" Generates a report summarizing the dataset features, solver features, and results.\n",
    "        Parameters:\n",
    "            X -- feature matrix (num_samples,num_features)\n",
    "            y -- target vector containing standard (int >= 0) class labels (num_samples, 1) \n",
    "            yval -- validation target vector (1-int(fts*num_samples), 1)\n",
    "            score_tr -- number of accurately predicted labels over the number of all predictions on the training set\n",
    "                        (float, min:0, max:1)\n",
    "            score_val -- number of accurately predicted labels over the number of all predictions on the validation set\n",
    "                        (float, min:0, max:1)\n",
    "            cost_val -- validation cost based on cross-entropy loss (-ylog(AL)-(1-y)log(1-AL))) + regularization term \n",
    "                        (float)\n",
    "        \"\"\"  \n",
    "        ### data\n",
    "        print (\"\\ndata:\")\n",
    "        print(\"\\tfile:\",self.pp.datafile)\n",
    "        m,n = X.shape\n",
    "        print (\"\\tm =\", m, \"training examples\")\n",
    "        print (\"\\tn =\", n, \"features\")\n",
    "        print(\"\\toriginal classes:\", self.pp.original_labels_)\n",
    "        for i in range(self.pp.numclass_):\n",
    "            z = np.where(y==i)[0]\n",
    "            print('\\tnumber of samples in class '+str(i)+\" =\", len(z)) \n",
    "        ### solver          \n",
    "        print (\"solver:\")\n",
    "        print (\"\\tfeature normalization: yes\") if self.pp.normalize == True else print (\"\\tfeature normalization: no\")            \n",
    "        if self.pp.polydeg > 1:\n",
    "            print (\"\\tpolynomial regression: yes (deg = \", str(self.pp.polydeg)+\")\")\n",
    "        elif self.pp.polydeg == 1:\n",
    "            print (\"\\tpolynomial regression: no\")\n",
    "        X1 = self.ds.add_bias(X)\n",
    "        print (\"\\tneural network config:\", self.ds.set_layer_sizes(X1,y))\n",
    "        if self.ds.lamda > 0:\n",
    "            print (\"\\tregularization: yes (lambda =\",str(self.ds.lamda)+\")\")\n",
    "        elif self.ds.lamda == 0:\n",
    "            print (\"\\tregularization: no\")\n",
    "        print (\"\\tmethod:\", self.ds.method)\n",
    "        if self.ds.method == 'GD':\n",
    "            print (\"\\tlearning rate =\", round(self.ds.lrfinal_,8))  \n",
    "        ### results    \n",
    "        print (\"output:\")\n",
    "        print (\"\\t* \"+str(self.ds.message_))\n",
    "        if self.ds.niter_ >= 1:            \n",
    "            print(\"\\tinitial cost =\", self.ds.Jh_[0])\n",
    "            print (\"\\tfinal cost =\", self.ds.Jh_[-1])     \n",
    "            print (\"\\tnumber of iterations =\", self.ds.niter_)\n",
    "            print(\"\\taccuracy on the training set:\",np.round(score_tr,3))\n",
    "            print (\"\\texecution time: \",self.ds.timetofit_,\" sec\")\n",
    "            if self.pp.partition == True:\n",
    "                    print (\"validation:\")   \n",
    "                    for i in range(len(np.unique(yval))):\n",
    "                        z = np.where(yval==i)[0]\n",
    "                        print('\\tnumber of samples in class '+str(i)+\" =\", len(z)) \n",
    "                    print (\"\\tcost =\", cost_val)            \n",
    "                    print(\"\\taccuracy on the validation set:\",np.round(score_val,3))\n",
    "                \n",
    "    def combine(self, seedno=0, plot_input=False, plot_J=False, plot_output=False, report_summary=False): \n",
    "        \"\"\" Combines the functions from classes PreProcess(), DeepSolve(), and plotting/reporting functions from class \n",
    "            DeepCombine() to pre-process input data, find parameters, optionally plot input/cost/decision boundary, and \n",
    "            generate a summary report.\n",
    "        Parameters:\n",
    "            seedno -- seed for randomization of training and validation set partition (int) \n",
    "            plot_input -- boolean that allows the dataset to be plotted if True (default=False) \n",
    "            plot_J -- boolean that allows the cost vs iterations to be plotted if True (default=False) \n",
    "            plot_output -- boolean that allows the decision boundary to be plotted if True (default=False) \n",
    "            report_summary -- boolean that allows the summary report to be generated if True (default=False) \n",
    "        Returns:\n",
    "            results -- dictionary storing network parameters theta ('theta'), training score ('score_tr'), \n",
    "                       validation score (score_val'), number of iterations ('numiter'), final cost ('finalcost'), \n",
    "                       and execution time ('time')            \n",
    "        \"\"\"\n",
    "        X,y,Xval,yval = self.pp.process_training_set(seedno)        \n",
    "        if plot_input == True:    \n",
    "            self.plt_input(X,y)\n",
    "            plt.show();\n",
    "        theta = self.ds.fit_nn(X,y)    \n",
    "        if self.ds.niter_  >= 1:      \n",
    "            pred_tr  = self.ds.predict(theta,X)\n",
    "            score_tr = self.ds.score(pred_tr,y)       \n",
    "            if self.pp.partition == True:\n",
    "                pred_val = self.ds.predict(theta,Xval)\n",
    "                score_val = self.ds.score(pred_val,yval)\n",
    "                Xval1 = self.ds.add_bias(Xval)\n",
    "                cost_val = self.ds.cost_nn(theta, Xval1, yval)\n",
    "            else:\n",
    "                score_val = None\n",
    "                cost_val = None            \n",
    "            results = {'theta':theta, 'score_tr':score_tr, 'score_val':score_val, 'numiter':self.ds.niter_, \n",
    "                       'finalcost': self.ds.Jh_[-1], 'timetofit': self.ds.timetofit_}            \n",
    "            if plot_J == True:\n",
    "                self.plot_J()\n",
    "                plt.close()           \n",
    "            if report_summary == True:\n",
    "                self.report(X,y,yval,score_tr, score_val,cost_val)        \n",
    "            if plot_output == True:\n",
    "                self.plt_output(X,y,theta,pred_tr)                \n",
    "                plt.close()           \n",
    "        elif self.ds.niter_ < 1:\n",
    "            score_tr  = None\n",
    "            score_val = None\n",
    "            cost_val = None\n",
    "            results = None                      \n",
    "        return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeepLearnAuto():    \n",
    "    \"\"\" Augments the capabilities of class DeepCombine() by automated tuning of polynomial degree, hidden-unit \n",
    "        configuration, and regularization parameter lamda. In addition, predicts classes on the test set and calculates\n",
    "        the test score.\n",
    "    Parameters:\n",
    "        datafile -- .csv file including feature matrix and target vector\n",
    "        normalize -- boolean that allows data normalization if True (default = False)        \n",
    "        polydeg -- boolean that allows feature matrix to be augmented with polynomial terms if > 1 (int, default = 1)        \n",
    "        method -- batch solver based on either a custom gradient descent implementation ('GD' which is the default) \n",
    "                  or other built-in algorithm implementations under scipy.optimize.minimize (such as 'TNC','CG',\n",
    "                  'SLSQP','BFGS','L-BFGS-B') \n",
    "                  See https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html.\n",
    "        initialize -- type of initialization for network parameters. \"zeros\" results in initial parameters being all \n",
    "                      zero. \"deep\" (default) results in He initialization\n",
    "        learning_rate -- learning rate for gradient descent. Effective only if method = 'GD'. Can be either a float \n",
    "                         > 0 or 'auto' (default) which would allow 'GD' to auto-select the learning rate\n",
    "        lamda -- regularization parameter (float >= 0, default = 0)\n",
    "        maxiter -- max number of iterations (int >= 1, default = 1000)\n",
    "        hidden units -- list showing number of units in each hidden layer. As an example, hidden_units = [20,15,10] \n",
    "                        refers to a network with three hidden layers and 20 units in the first layer and so on. \n",
    "                        hidden_units = [] (default) means there are no hidden units (in which case the operation is \n",
    "                        simple logistic regression)        \n",
    "        num_hidden_layers -- number of hidden layers (with equal number of hidden units) for selection of optimal \n",
    "                             hidden unit configuration in auto mode (int > 0, default = 1)\n",
    "        total_units_max -- total number of hidden units for selection of optimal hidden unit configuration in auto\n",
    "                           mode. As an example, if num_hidden_layers = 2 and total_units_max = 10, the candidate hidden \n",
    "                           layers for two classes are [] (which is the reference), [3,3], and [4,4]. \n",
    "                           (int > 0, default=10)        \n",
    "        plot_input -- boolean that allows the dataset to be plotted if True (default=False)\n",
    "        plot_J -- boolean that allows the normalized cost vs iterations to be plotted if True (default=False)\n",
    "        report_summary -- boolean that allows the summary report to be generated if True (default=False)\n",
    "        plot_output -- boolean that allows the decision boundary to be plotted if True (default=False)\n",
    "        plot_test -- boolean that allows the decision boundary to be plotted for the test set if True (default=False)\n",
    "        plot_poly -- boolean that allows training score vs polynomial degree to be plotted if True (default=False) \n",
    "        plot_hidden -- boolean that allows training score vs hidden units config to be plotted if True (default=False)\n",
    "        plot_lam -- boolean that allows training and validation scores vs lamda to be plotted if True\n",
    "        polydeg_max -- max candidate polynomial degree to consider while selecting the optimal degree in auto mode \n",
    "                      (int >=1, fixed at 5)\n",
    "        lamda_numdecade -- number of decades for optimal lamda search (int >= 1, fixed at 3)\n",
    "        totalseed -- number of times dataset is randomly partitioned based on different seeds (int >= 1, fixed at 10)\n",
    "        topscore -- limit (\"good enough\") score to stop the search for polydeg or hidden units (float, fixed at 0.95)\n",
    "        deltascore -- minimum training score improvement needed to continue the search for a higher polynomial degree \n",
    "                      or total number of hidden units. The idea is to stop searching for more complex models if the \n",
    "                      training score is already saturated (float, fixed at 0.05)         \n",
    "        self.valscoredelta -- during the optimal lamda search, this is the minimum validation score improvement needed \n",
    "                              to continue the search for a lower lamda. The idea is to go to lower lamda values only if \n",
    "                              validation score is going to improve. Otherwise we prefer to keeplamda high (float, fixed\n",
    "                              at 0.005)\n",
    "        self.deltatrscorevslam0 -- during the optimal lamda search, this is the maximum allowable difference between \n",
    "                                   the training score for a candidate lamda and training score for lamda=0 (float, \n",
    "                                   fixed at 0.05)\n",
    "    Attributes:\n",
    "        lamda_array_ -- a subset array of candidate lamda varying logarithmically between 3 and the candidate lamda \n",
    "                        that is next to (and smaller than) the optimal lamda (num_subset_lamda_cand,) \n",
    "        valscore_mean_ -- list of average validation scores after totalseed numbers of shuffling for each element of \n",
    "                          lamda_array_\n",
    "        trscore_mean_ -- list of average validation scores after totalseed numbers of shuffling for each element of \n",
    "                         lamda_array_\n",
    "        \n",
    "    \"\"\"    \n",
    "    def __init__(self,datafile='', normalize=True, polydeg='auto', \n",
    "                 method='GD', initialize = 'deep', learning_rate='auto', lamda='auto', maxiter=1000,  \n",
    "                 hidden_units='auto', num_hidden_layers=1, total_units_max=10,  \n",
    "                 plot_input=False, plot_J=False, report_summary=False, plot_output=False, plot_test=False,                  \n",
    "                 plot_poly=False, plot_hidden=False, plot_lam=False):\n",
    "        self.datafile = datafile\n",
    "        self.normalize = normalize\n",
    "        self.polydeg = polydeg\n",
    "        self.method = method \n",
    "        self.initialize = initialize\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lamda = lamda\n",
    "        self.maxiter = maxiter\n",
    "        self.hidden_units = hidden_units        \n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.total_units_max = total_units_max\n",
    "        self.plot_input = plot_input\n",
    "        self.plot_J = plot_J\n",
    "        self.report_summary = report_summary\n",
    "        self.plot_output = plot_output \n",
    "        self.plot_test = plot_test\n",
    "        self.plot_poly = plot_poly\n",
    "        self.plot_hidden = plot_hidden\n",
    "        self.plot_lam = plot_lam\n",
    "        self.polydeg_max=5\n",
    "        self.lamda_numdecade=3\n",
    "        self.totalseed=10 \n",
    "        self.topscore=0.95 \n",
    "        self.deltascore=0.05\n",
    "        self.valscoredelta = 0.005\n",
    "        self.deltatrscorevslam0 = 0.05\n",
    "   \n",
    "    def deepcombine(self, poly=1, lam=0, part=False, hu=[]): \n",
    "        \"\"\" Returns an instance of class DeepCombine() where the class parameters polydeg, lamda, partition, and \n",
    "            hidden_units were employed as function parameters poly, lam, part, hu. This is a supporting function that \n",
    "            allows automated selection of optimal polydeg, lamda, and hidden_units parameters by other functions of \n",
    "            class DeepLearnAuto()(which would not be possible if class DeepCombine() were to be instantiated under \n",
    "            __init__().)\n",
    "        Parameters:\n",
    "            poly -- boolean that allows feature matrix to be augmented with polynomial terms if > 1 (int, default = 1)\n",
    "            lam -- regularization parameter (float >= 0, default = 0)\n",
    "            part -- boolean that allows partitioning of the dataset into training and validation sets if True \n",
    "                    (default = False)\n",
    "            hu -- list showing number of units in each hidden layer\n",
    "        Returns:\n",
    "            dc_instance -- an instance of class DeepCombine() allowing to flexibly set parameters polydeg, lamda, \n",
    "                           partition, and hidden_units\n",
    "        \"\"\"\n",
    "        dc_instance = DeepCombine(\n",
    "                      datafile = self.datafile,\n",
    "                      normalize = self.normalize,\n",
    "                      polydeg = poly,\n",
    "                      partition = part,\n",
    "                      method = self.method,  \n",
    "                      initialize  = self.initialize,\n",
    "                      learning_rate = self.learning_rate,\n",
    "                      lamda = lam,\n",
    "                      maxiter = self.maxiter,                \n",
    "                      hidden_units = hu)\n",
    "        return dc_instance\n",
    "\n",
    "    def num_class(self):\n",
    "        \"\"\" Returns number of classes in the dataset.\n",
    "        Returns:\n",
    "            number_classes -- number of classes associated with the target vector (int, > 1)\n",
    "        \"\"\"\n",
    "        temp_instance = self.deepcombine() #initialize this object just to get pp.numclass_\n",
    "        temp_instance.pp.process_training_set();\n",
    "        number_classes = temp_instance.pp.numclass_\n",
    "        return number_classes\n",
    "    \n",
    "    def candidate_poly(self):\n",
    "        \"\"\" Generates a list of candidate polynomial degrees.\n",
    "        Returns:\n",
    "            cand_poly -- list of integers from 1 to polydeg_max\n",
    "        \"\"\"\n",
    "        cand_poly = (np.arange(self.polydeg_max)+1).tolist()\n",
    "        return cand_poly\n",
    "    \n",
    "    def candidate_lamda(self): \n",
    "        \"\"\" Generates a list of candidate lambda parameters for regularization.\n",
    "        Returns:\n",
    "            cand_lamda -- list of floats in descending order varying as .. 0.1, 0.03, 0.01 (= minimum candidate lamda \n",
    "                          value) for a number of decades set by parameter lamda_numdecade. Max value = 3 for \n",
    "                          lamda_numdecade = 3\n",
    "        \"\"\"\n",
    "        cand_lamda = -np.sort(-np.append(np.logspace(1,self.lamda_numdecade,self.lamda_numdecade)/1000, \n",
    "                                         np.logspace(1,self.lamda_numdecade,self.lamda_numdecade)*3/1000)) \n",
    "                    # \n",
    "        return cand_lamda \n",
    "    \n",
    "    def candidate_hidden_units(self):\n",
    "        \"\"\" Generates a list of candidate hidden unit configurations. The number of hidden layers is set by parameter\n",
    "            num_hidden_layers. Each hidden layer has the same number of units. The minimum number of units is num_class    + 1. \n",
    "            + 1. The maximum number of units is set by parameter total_units_max such that the total number of units in all hidden \n",
    "            all layers combined cannot exceed total_units_max. As an example, if num_class = 3, num_hidden_layers = 3 \n",
    "            and total_units_max = 16, the candidate hidden_layers would be [[4,4,4],[5,5,5]]. In addition, an empty \n",
    "            list, which corresponds to logistic regression reference is always added as a candidate.\n",
    "        Returns:\n",
    "            cand_hid_lay -- list of candidate hidden layer configurations which themselves are lists\n",
    "        \"\"\"\n",
    "        a = []        \n",
    "        def b(i):\n",
    "            return (np.ones((1,self.num_hidden_layers))*(self.num_class()+1+i)).tolist()[0]\n",
    "            # b(i) returns a list that is num_hidden_layers long and each element is equal to nc+1+i. \n",
    "            # Since i >=0, this means the candidate number of units will always be at least 1 higher than num_class\n",
    "        i = 0\n",
    "        while 0 < np.sum(b(i)) <= self.total_units_max: # total number of units cannot exceed total_units_max             \n",
    "            c = list(map(int, b(i)))\n",
    "            a.append(c) # makes each element of list b(i) an integer\n",
    "            i += 1\n",
    "        cand_hid_lay = [[]] + a # always add \"no hidden layers\" case (logistic regression) as a candidate\n",
    "        return cand_hid_lay\n",
    "        \n",
    "    def scores_vs_poly_or_hu(self, cand_poly_or_hu):\n",
    "        \"\"\" Generates the classification scores for a list of candidate hyperparameters which can be either polynomial \n",
    "            degree or hidden units. Auto-detects whether search to be done for polydeg or hidden units based on the \n",
    "            input parameter format.\n",
    "        Parameters:\n",
    "            cand_poly_or_hu -- a list of integers if candidate hyperparameter is polynomial degree, e.g., [1,2,3,4]\n",
    "                               a list of lists if candidate hyperparameter is hidden units, e.g., [[],[3,3],[4,4]]\n",
    "        Returns:\n",
    "            sum_arr_sorted -- np array showing scores vs candidate hyperparameters in ascending order \n",
    "                             (num_final_cand_param,2) \n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        param = []\n",
    "        scr = 0\n",
    "        i = 0\n",
    "        while i < len(cand_poly_or_hu) and scr < self.topscore: # stop if topscore is exceeded before going through all \n",
    "                                                                # the candidates. topscore can be selected just 1 or as\n",
    "                                                                # in our case < 1 to potentially end up with a faster \n",
    "                                                                # selection and a smaller number of candidates. topscore \n",
    "                                                                # < 1 may also have a regularization impact.    \n",
    "            if type(cand_poly_or_hu[0]) is list:  # this means cand_poly_or_hu is hidden_units\n",
    "                if self.polydeg == 'auto':\n",
    "                    pol = 1\n",
    "                else:\n",
    "                    pol = self.polydeg\n",
    "                dc = self.deepcombine(poly=pol,lam=0,part=False, hu = cand_poly_or_hu[i]) # if polydeg='auto', compute \n",
    "                                                                                          # scores vs hidden_units for \n",
    "                                                                                          # polydeg=1 & lam=0, w/o \n",
    "                                                                                          # partitioning the data\n",
    "                    \n",
    "            else:  # this means cand_poly_or_hu is polydeg\n",
    "                if self.hidden_units == 'auto':\n",
    "                    hU = [] \n",
    "                else:\n",
    "                    hU = self.hidden_units\n",
    "                dc = self.deepcombine(poly=cand_poly_or_hu[i], lam=0, part=False, hu=hU) # if  hidden_units='auto', \n",
    "                                                                                         # compute scores vs polydeg for \n",
    "                                                                                         # hidden_units=[] & lam=0 w/o \n",
    "                                                                                         # partitioning the data\n",
    "            solve = dc.combine();                                    \n",
    "            if solve != None:\n",
    "                scr = solve['score_tr']\n",
    "                scores.append(scr)\n",
    "                param.append(cand_poly_or_hu[i])                \n",
    "            else:\n",
    "                para = 'hidden units' if isinstance(cand_poly_or_hu[i], list) == True else 'poly deg'\n",
    "                print ('no convergence for '+str(para)+\" =\",cand_poly_or_hu[i])\n",
    "            i+=1\n",
    "        if len(param) == 0:\n",
    "            print (\"\\nNone of the parameters converged within the search range.\\n\")\n",
    "        assert len(param) > 0 # abort if no convergence for any candidate polydeg or hidden unit configuration         \n",
    "        if param == [[]]: # handle special case \n",
    "            summ_arr_sorted = np.array(([],np.array(scores))).reshape((1,2)) # otherwise column_stack does not work\n",
    "        else:            \n",
    "            summ_arr = np.column_stack((np.array(param), np.array(scores)))\n",
    "            summ_arr_sorted = summ_arr[np.argsort(summ_arr[:, 1])] # sort polydeg or hidden_units by their score\n",
    "        return summ_arr_sorted\n",
    "\n",
    "    def find_best_poly_or_hu(self, summ_arr_sorted):\n",
    "        \"\"\" Finds the optimal polydeg or hidden units configuration starting with the candidate array sorted by \n",
    "            scores (provided by scores_vs_poly_or_hu function.) However the optimal candidate is not simply the one \n",
    "            with the highest score. Another candidate with slightly lower score is preferred if it is less complex \n",
    "            (lower polydeg or less number of units.) Algorithm here starts with highest ranked candidate and moves  \n",
    "            to those with lower scores as long as the score difference is lower than parameter deltascore. Once a \n",
    "            subset of candidates with scores close enough to the highest score is determined, the candidate with \n",
    "            minimum complexity out of that subset is selected as the optimal parameter. \n",
    "        Parameters:\n",
    "            sum_arr_sorted -- np array showing scores vs candidate hyperparameters in ascending order \n",
    "                             (num_final_cand_param,2) \n",
    "        Returns:\n",
    "            bestparam -- optimal polydeg (integer) or optimal hidden units configuration (list of integers)        \n",
    "        \"\"\"\n",
    "        s = len(summ_arr_sorted)\n",
    "        param = summ_arr_sorted[:,0]\n",
    "        scores = summ_arr_sorted[:,1]\n",
    "        paramsum = []\n",
    "        for i in range (0,s):\n",
    "            paramsum.append(np.sum(param[i])) # paramsum = param for polydeg or hidden units with single hidden unit.                \n",
    "        if s == 1:\n",
    "            bestparam = param[0] # if only one candidate available that one is automatically the best\n",
    "        elif s > 1:\n",
    "            highest_score = scores[-1] # last score is the highest since the array was sorted\n",
    "            idx = s-1\n",
    "            while (highest_score - scores[idx]) < self.deltascore and idx > 0:\n",
    "                idx -= 1 # If the score difference from one param to next is not bigger than parameter deltascore, this\n",
    "                         # means complexity can be reduced (polydeg can be decreased or a smaller number of units per \n",
    "                         # layer can be used) with a relatively small penalty in acuracy. This is equivalent to \n",
    "                         # increasing regularization parameter lamda in that it would tend to prevent overfitting.\n",
    "            sum_better_scores = paramsum[idx+1:s] # sum is being used as a measure of complexity for hidden layers \n",
    "                                                  # with more than one unit  \n",
    "            param_better_scores = param[idx+1:s] # this is a subset of parameters with better scores. The reason why\n",
    "                                                 # param[idx+1] does not simply become the best parameter is because\n",
    "                                                 # within the param_better_scores subset, there may actually be a \n",
    "                                                 # a smaller polydeg or hidden layers with less number of units and yet\n",
    "                                                 # with higher score\n",
    "            idx_min_sum = np.argmin(sum_better_scores)\n",
    "            bestparam = param_better_scores[idx_min_sum]\n",
    "        return bestparam\n",
    "       \n",
    "    def plot_scores_vs_poly_or_hu(self, summ_arr_sorted, bestparam): \n",
    "        \"\"\" Plots scores vs candidate poly degs or vs candidate hidden unit configurations and highlights the optimal\n",
    "            parameter.\n",
    "        Parameters:\n",
    "            sum_arr_sorted -- np array showing scores vs candidate hyperparameters in ascending order \n",
    "                              (num_final_cand_param,2) \n",
    "            bestparam -- optimal polydeg (integer) or optimal hidden units configuration (list of integers)                         \n",
    "        \"\"\"\n",
    "        param = summ_arr_sorted[:,0].tolist()\n",
    "        scores = summ_arr_sorted[:,1]\n",
    "        x = np.arange(0,len(summ_arr_sorted),1).tolist() # x: [0,1,2..len(scores)-1]\n",
    "        plt.plot(x,scores) # scores vs x\n",
    "        plt.scatter(x,scores, s=50, c='r', edgecolor='red') # overlay scatter data pts\n",
    "        index_bestparam = param.index(bestparam)\n",
    "        score_bestparam = scores[index_bestparam]\n",
    "        x_bestparam = x[index_bestparam]\n",
    "        plt.scatter(x_bestparam, score_bestparam, s=100, facecolor=\"None\", edgecolor='black', linewidth=3) # highlight \n",
    "                                                                                                           # bestparam        \n",
    "        plt.xlim(-1,len(scores))\n",
    "        LABELS = param\n",
    "        if type(param[0]) is list: \n",
    "            xlab = 'hidden units'\n",
    "            plt.xticks(x, LABELS, rotation=90) # replace x with candidate hidden units\n",
    "        else:\n",
    "            xlab = 'polynomial degree' \n",
    "            plt.xticks(x, map(int,LABELS)) # replace x with candidate poly degs\n",
    "        plt.xlabel(xlab)\n",
    "        plt.ylabel('score')\n",
    "        plt.title(\"score vs \"+xlab)\n",
    "        plt.show() \n",
    "    \n",
    "    def mean_scores_of_shuffled_data(self, polydeg, hidden_units, lamda):\n",
    "        \"\"\" Partitions the dataset randomly and differently for the number of times set by parameter totalseed, \n",
    "            calculates the training and validation scores for each partitioning and returns the average of them.\n",
    "            The purpose is to minimize the variation of training and validation scores due to random partitioning. \n",
    "        Parameters:\n",
    "            polydeg -- boolean that allows feature matrix to be augmented with polynomial terms if > 1 (int, default=1)\n",
    "            hidden_units -- list showing number of units in each hidden layer\n",
    "            lamda -- regularization parameter (float >= 0, default = 0) \n",
    "        Returns:\n",
    "            mean_val_score -- validation set average score after the dataset is randomly partitioned by totalseed \n",
    "                              number of times (float, min:0, max:1)                             \n",
    "            mean_tr_score -- training set average score after the dataset is randomly partitioned by totalseed \n",
    "                             number of times (float, min:0, max:1) \n",
    "        \"\"\"    \n",
    "        valscores, trscores = [], []\n",
    "        for seed in range(0,self.totalseed):           \n",
    "            dc = self.deepcombine(poly=polydeg,lam=lamda,part=True,hu=hidden_units)\n",
    "            results = dc.combine(seedno=seed);                                    \n",
    "            if results != None:        \n",
    "                valscores.append(results['score_val']) \n",
    "                trscores.append(results['score_tr'])\n",
    "            else:\n",
    "                print ('no convergence for lamda =',lamda, \"seed =\", seed)\n",
    "        if len(trscores) == 0:\n",
    "            print(\"\\nNo convergence for any of the attempted seeds while lamda =\", lamda, \"\\n\")\n",
    "        assert len(trscores) != 0\n",
    "        mean_val_score, mean_tr_score = np.mean(valscores), np.mean(trscores)\n",
    "        return mean_val_score, mean_tr_score\n",
    "        \n",
    "    def find_bestlambda(self, bestpoly, besthu):\n",
    "        \"\"\" Selects the optimal lambda. Starts by collecting mean training and validation scores for lamda = 10. Then\n",
    "            reduces lamda logarithmically as long as validation score improves by an amount higher than the parameter \n",
    "            valscoredelta OR training score is too small (i.e., difference between training score for the actual lamda \n",
    "            and training score for lamda=0 is higher than the parameter deltatrscorevslam0.) Optimal lamda is the last \n",
    "            one before both these conditions become invalid. If the conditions become valid down to the smallest \n",
    "            lamda, which is 0.01, then the optimal lamda is 0.01.            \n",
    "        Parameters:\n",
    "            bestpoly -- optimal polydeg (integer)\n",
    "            besthu -- optimal hidden units configuration (list of integers)    \n",
    "        Returns:\n",
    "            bestlambda -- optimal lamda (float, >= 0.01)\n",
    "        \"\"\"\n",
    "        ref_scores = self.mean_scores_of_shuffled_data(bestpoly,besthu,lamda=0) \n",
    "        trscore0_mean=ref_scores[1] # tr score for lamda=0\n",
    "        lamda_cand = self.candidate_lamda()\n",
    "        laminit = 10 # next one higher than the nominal max, which is 3 for lamda_numdecade = 3\n",
    "        init_scores = self.mean_scores_of_shuffled_data(bestpoly,besthu,lamda=laminit)\n",
    "        self.valscore_mean_, self.trscore_mean_=[init_scores[0]],[init_scores[1]]  \n",
    "        lamd=[laminit]\n",
    "        i = 0\n",
    "        while i < len(lamda_cand): # the highest candidate is 3, lowest 0.01\n",
    "            scores = self.mean_scores_of_shuffled_data(bestpoly,besthu,lamda_cand[i])            \n",
    "            self.valscore_mean_.append(scores[0])\n",
    "            self.trscore_mean_.append(scores[1]) \n",
    "            lamd.append(lamda_cand[i])            \n",
    "            if (self.valscore_mean_[i+1]-self.valscore_mean_[i] > self.valscoredelta or \n",
    "                trscore0_mean - self.trscore_mean_[i+1] > self.deltatrscorevslam0):\n",
    "            # reduce lamda as long as valscore improvement is above certain threshold (%0.5, just to have it slightly \n",
    "            # above 0.) The 2nd condition (following or) prevents a case where there is no validation score improvement \n",
    "            # for decreasing lamda while lamda is very large (e.g., valscore being flat from lam = 10 to 3)\n",
    "                i += 1\n",
    "            else:\n",
    "                break\n",
    "        self.lamda_array_ = np.array(lamd)\n",
    "        bestlamda = self.lamda_array_[-2] \n",
    "        return bestlamda\n",
    "         \n",
    "    def plot_scores_vs_lamda(self):\n",
    "        \"\"\" Plots the average training and validation scores as a function of a subset of candidate lamda. \n",
    "            Training and validation scores corresponding to optimal lamda were also highlighted.\n",
    "        \"\"\"\n",
    "        lamda = self.lamda_array_\n",
    "        valscores = self.valscore_mean_\n",
    "        trscores = self.trscore_mean_\n",
    "        bestvalscore = self.valscore_mean_[-2]\n",
    "        besttrscore = self.trscore_mean_[-2]\n",
    "        bestlambda = self.lamda_array_[-2]\n",
    "        plt.semilogx(lamda,valscores,label='val')\n",
    "        plt.semilogx(lamda,trscores,label='tr')            \n",
    "        plt.scatter(lamda,valscores, s=50, c='r', edgecolor = 'r')\n",
    "        plt.scatter(lamda,trscores, s=50, c='blue', edgecolor='blue')\n",
    "        plt.scatter(bestlambda, bestvalscore, s=100, facecolor=\"None\", edgecolor='black', linewidth=3)        \n",
    "        plt.scatter(bestlambda, besttrscore, s=100, facecolor=\"None\", edgecolor='black', linewidth=3)\n",
    "        plt.xscale('log')\n",
    "        plt.legend(loc='best')\n",
    "        plt.xlabel(\"lambda\")\n",
    "        plt.ylabel(\"scores\")\n",
    "        plt.title(\"scores vs lambda\")\n",
    "        plt.show() \n",
    "    \n",
    "    def hyperparams(self):\n",
    "        \"\"\" Combines functions to compute optimal polydeg, hidden units configuration, and lamda in auto mode and\n",
    "            displays optionally the associated plots. If all three parameters are in the auto mode, optimal polydeg \n",
    "            is calculated first, then the optimal hidden units configuration is calculated by using the optimal polydeg\n",
    "            and finally the optimal lamda is calculated using the optimal polydeg and optimal hidden units. If any of \n",
    "            these hyperparameters is not in auto mode, the value defined by the user is employed. \n",
    "        Returns:\n",
    "            bestpoly -- optimal polydeg (integer)\n",
    "            besthu -- optimal hidden units configuration (list of integers) \n",
    "            bestlamda -- optimal lamda (float, >= 0.01)\n",
    "        \"\"\"\n",
    "        if self.polydeg == 'auto':\n",
    "            cand = self.candidate_poly()\n",
    "            arr_poly = self.scores_vs_poly_or_hu(cand)            \n",
    "            bestp = self.find_best_poly_or_hu(arr_poly)\n",
    "            bestpoly = np.int(bestp)\n",
    "            self.polydeg = bestpoly # so that arr_hu = self.scores_vs_poly_or_hu(cand) below uses bestpoly value\n",
    "            if self.plot_poly == True:\n",
    "                self.plot_scores_vs_poly_or_hu(arr_poly, bestpoly) \n",
    "        else:\n",
    "            bestpoly = self.polydeg         \n",
    "        if self.hidden_units == 'auto':\n",
    "            cand = self.candidate_hidden_units()\n",
    "            arr_hu = self.scores_vs_poly_or_hu(cand)\n",
    "            besthu = self.find_best_poly_or_hu(arr_hu)\n",
    "            if self.plot_hidden == True:\n",
    "                self.plot_scores_vs_poly_or_hu(arr_hu, besthu) \n",
    "        else:           \n",
    "            besthu = self.hidden_units        \n",
    "        if self.lamda == 'auto':\n",
    "            bestlamda = self.find_bestlambda(bestpoly,besthu)\n",
    "            if self.plot_lam == True:\n",
    "                self.plot_scores_vs_lamda()\n",
    "        else:\n",
    "            bestlamda = self.lamda\n",
    "        return bestpoly, besthu, bestlamda\n",
    "    \n",
    "    def fit_auto(self):\n",
    "        \"\"\" First calculates optimal hyperparameters that are in auto mode. Next, using those hyperparameters, finds\n",
    "            theta with an additional round of iterations while plotting optionally related graphs and creating the \n",
    "            summary report. In addition to returning the number of iteraions, execution time, and the final cost, if \n",
    "            the training set is normalized, makes available its mean and sigma values, which can be used for prediction\n",
    "            on the test set.\n",
    "        Returns:\n",
    "            theta -- dictionary holding network parameters\n",
    "                     keys: int > 0, total number of keys: num_layers - 1\n",
    "                     dimensions for each key: (num_units_next, num_units_prev+1)\n",
    "            score_tr -- number of accurately predicted labels over the number of all predictions on the training set\n",
    "                        (float, min:0, max:1) \n",
    "            bestpoly -- optimal polydeg (integer)\n",
    "            besthu -- optimal hidden units configuration (list of integers) \n",
    "            bestlamda -- optimal lamda (float, >= 0.01)\n",
    "            niter -- number of iterations (int > 0)\n",
    "            timetofit -- execution time taken by solver (sec)\n",
    "            finalcost -- cost after the last iteration\n",
    "            mu_tr -- array containing normalization mean from the training set, an empty list in case taining set is \n",
    "                     not normalized (num_features,)  \n",
    "            sigma_tr -- array containing normalization standard deviation from the training set, an empty list in case \n",
    "                        training set is not normalized (num_features,)\n",
    "        \"\"\"\n",
    "        bestpoly, besthu, bestlambda = self.hyperparams();\n",
    "        best_inst = self.deepcombine(poly = bestpoly,lam = bestlambda, part=False, hu = besthu) \n",
    "        param = best_inst.combine(plot_input=self.plot_input, report_summary=self.report_summary, \n",
    "                                  plot_output=self.plot_output, plot_J=self.plot_J);                            \n",
    "        theta, score_tr, niter, timetofit, finalcost = (param['theta'], param['score_tr'], param['numiter'], \n",
    "                                                       param['timetofit'], param['finalcost'])\n",
    "        if self.normalize==True:\n",
    "            mu_tr = best_inst.pp.mu_\n",
    "            sigma_tr = best_inst.pp.sigma_\n",
    "        else:\n",
    "            mu_tr = []\n",
    "            sigma_tr = []\n",
    "        return theta, score_tr, bestpoly, besthu, bestlambda, niter, timetofit, finalcost, mu_tr, sigma_tr\n",
    "    \n",
    "    def pred_score_test(self, test_file, theta, bestpoly, besthu, mu_tr,sigma_tr):\n",
    "        \"\"\" Pre-processes the test set based on whether the training set was normalized or had used a polydeg > 1. \n",
    "            Then, using theta and hidden units configuration, makes prediction on the test set and finally provides\n",
    "            the score associated with the test set.\n",
    "        Parameters:\n",
    "            test_file -- test data set never seen by the algorithm while theta is found \n",
    "                        (num_test_examples, original number of features in the training set)           \n",
    "            theta -- dictionary holding network parameters\n",
    "                     keys: int > 0, total number of keys: num_layers - 1\n",
    "                     dimensions for each key: (num_units_next, num_units_prev+1)\n",
    "            bestpoly -- optimal polydeg (integer)\n",
    "            besthu -- optimal hidden units configuration (list of integers) \n",
    "            mu_tr -- array containing normalization mean from tr set, an empty list in case tr set is not normalized\n",
    "                    (num_features,)  \n",
    "            sigma_tr -- array containing normalization std dev from tr set, an empty list in case tr set is not \n",
    "                        normalized (num_features,)\n",
    "        Returns:\n",
    "            pred_test -- array of predicted output on the test set using theta that was determined by a separate \n",
    "                         training set. This is in standard form of int >= 0 (num_test_samples,)\n",
    "            score_test -- number of accurately predicted labels over the number of all predictions on the test set\n",
    "                          (float, min:0, max:1)                        \n",
    "        \"\"\"\n",
    "        best_inst = self.deepcombine(bestpoly,hu=besthu)\n",
    "        self.normalize == True if len(mu_tr)!=[] and len(sigma_tr)!=[] else False\n",
    "        Xtest, ytest = best_inst.pp.process_test_set(test_file,mu_tr,sigma_tr)\n",
    "        pred_test = best_inst.ds.predict(theta,Xtest);\n",
    "        if self.plot_test == True:\n",
    "            best_inst.plt_output(Xtest,ytest,theta,pred_test,tr=False)\n",
    "            plt.close()\n",
    "        score_test = best_inst.ds.score(pred_test,ytest)\n",
    "        score_test = np.round(score_test,3)\n",
    "        return pred_test,score_test\n",
    "\n",
    "    def generate_summary(self):\n",
    "        \"\"\" Combines fit_auto and pred_score_test methods to return a dataframe which displays essentially all\n",
    "            parameters of interest: datafile short name, normalization, calculation method (algorithm), number of \n",
    "            iterations, execution time, optimal polynomial degree, optimal hidden units configuration, optimal lamda, \n",
    "            training score, and test score (so an independent test file should be available.) If training set filename \n",
    "            is abc_tr.csv, test set should be named abc_test.csv and should be inserted under the same folder as the \n",
    "            training set.\n",
    "        Returns:\n",
    "            scoreboard_df -- summary table (dataframe)                      \n",
    "        \"\"\"\n",
    "        theta,score_tr, bestpoly, besthu, bestlamda, niter, timetofit, finalcost, mu_tr, sigma_tr = self.fit_auto()\n",
    "        ### datafile string manipulation to get the concise and correct filenames\n",
    "        slash_loc1 = np.char.find(self.datafile,'\\\\')\n",
    "        temp = self.datafile[slash_loc1+1:]\n",
    "        slash_loc2 = np.char.find(temp,'\\\\')\n",
    "        _loc = np.char.find(temp,'_tr')        \n",
    "        self.dataname = temp[slash_loc2+1:_loc]\n",
    "        testfile = self.datafile[:_loc+slash_loc1+1]+\"_test.csv\"\n",
    "        ###\n",
    "        ps_test = self.pred_score_test(testfile, theta, bestpoly, besthu, mu_tr, sigma_tr)\n",
    "        scoreboard = np.array((self.dataname,self.normalize, self.method, niter, timetofit, bestpoly,str(besthu), \n",
    "                               bestlamda, np.round(score_tr,3),ps_test[1].astype(str))).reshape((1,10))\n",
    "        cols = ['data','normalize','method','num iter','exec time','poly deg','hidden layers','lamda', 'tr score', \n",
    "                'test score']\n",
    "        scoreboard_df = pd.DataFrame(scoreboard,columns = cols)\n",
    "        return scoreboard_df"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
